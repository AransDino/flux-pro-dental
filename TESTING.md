# üß™ Sistema de Testing - AI Models Pro Generator

Este documento describe el sistema de testing automatizado implementado para el proyecto AI Models Pro Generator.

## üìã √çndice

- [Visi√≥n General](#visi√≥n-general)
- [Instalaci√≥n](#instalaci√≥n)
- [Ejecuci√≥n de Pruebas](#ejecuci√≥n-de-pruebas)
- [Estructura de Testing](#estructura-de-testing)
- [Reportes y Cobertura](#reportes-y-cobertura)
- [Configuraci√≥n Avanzada](#configuraci√≥n-avanzada)
- [Desarrollo de Nuevas Pruebas](#desarrollo-de-nuevas-pruebas)

## üéØ Visi√≥n General

El sistema de testing est√° construido con **pytest** y proporciona:

- ‚úÖ **40 pruebas automatizadas** cubriendo todos los componentes cr√≠ticos
- üìä **An√°lisis de cobertura** con reportes HTML y terminal
- üîß **Mocks y fixtures** para pruebas aisladas y confiables
- üì± **M√∫ltiples formas de ejecuci√≥n** (Python, batch, Makefile)
- üöÄ **Integraci√≥n continua** preparada para CI/CD

### Componentes Probados

| Componente | Archivo de Pruebas | Cobertura | Descripci√≥n |
|------------|-------------------|-----------|-------------|
| **Sistema de Historial** | `test_historial.py` | 96% | Carga, guardado, filtrado y validaci√≥n |
| **C√°lculos de Costo** | `test_cost_calculation.py` | 96% | Costos por modelo, conversiones, validaciones |
| **Integraci√≥n Replicate** | `test_replicate_integration.py` | 88% | API, generaci√≥n, manejo de errores |
| **Utilidades** | `test_utils.py` | 94% | Archivos, configuraci√≥n, helpers |

## üöÄ Instalaci√≥n

### Instalaci√≥n Autom√°tica

Para instalar todas las dependencias de testing autom√°ticamente:

```bash
# Windows (Batch)
run_tests.bat install

# Windows (PowerShell) o Linux/Mac
python run_tests.py install

# Usando Makefile
make dev
```

### Instalaci√≥n Manual

```bash
pip install pytest>=7.4.0 pytest-mock>=3.11.0 pytest-cov>=4.1.0 pytest-html>=3.2.0
```

## üß™ Ejecuci√≥n de Pruebas

### M√©todos de Ejecuci√≥n

#### 1. Script Python (Multiplataforma)
```bash
# Todas las pruebas
python run_tests.py

# Solo pruebas unitarias
python run_tests.py unit

# Con an√°lisis de cobertura
python run_tests.py coverage

# Solo generar reporte HTML
python run_tests.py report
```

#### 2. Script Batch (Windows)
```batch
# Todas las pruebas
run_tests.bat

# Pruebas espec√≠ficas
run_tests.bat unit
run_tests.bat coverage
run_tests.bat quick
```

#### 3. Makefile (Linux/Mac/Windows con make)
```bash
# Pruebas b√°sicas
make test

# Solo unitarias
make test-unit

# Con cobertura
make test-coverage

# Reporte HTML
make test-report

# Verificaci√≥n completa
make check
```

#### 4. Pytest Directo
```bash
# B√°sico
pytest tests/

# Con cobertura
pytest tests/ --cov=. --cov-report=html

# Solo un archivo
pytest tests/test_historial.py -v

# Solo una prueba espec√≠fica
pytest tests/test_cost_calculation.py::TestCostCalculation::test_calculate_flux_pro_cost
```

### Comandos R√°pidos

```bash
# Pruebas r√°pidas (para en el primer fallo)
pytest tests/ -x

# Pruebas paralelas (si tienes pytest-xdist)
pytest tests/ -n auto

# Modo verboso con traceback corto
pytest tests/ -v --tb=short

# Solo pruebas que contengan "cost" en el nombre
pytest tests/ -k "cost"
```

## üìÅ Estructura de Testing

```
tests/
‚îú‚îÄ‚îÄ __init__.py                     # Marcador de paquete
‚îú‚îÄ‚îÄ conftest.py                     # Fixtures globales y configuraci√≥n
‚îú‚îÄ‚îÄ pytest.ini                     # Configuraci√≥n de pytest
‚îú‚îÄ‚îÄ test_cost_calculation.py        # Pruebas de c√°lculos de costo
‚îú‚îÄ‚îÄ test_historial.py              # Pruebas del sistema de historial
‚îú‚îÄ‚îÄ test_replicate_integration.py  # Pruebas de integraci√≥n con Replicate
‚îî‚îÄ‚îÄ test_utils.py                  # Pruebas de funciones utilitarias
```

### Fixtures Disponibles

```python
# En conftest.py
@pytest.fixture
def temp_dir():          # Directorio temporal para pruebas
def sample_history():    # Datos de historial de prueba
def mock_replicate():    # Mock del cliente de Replicate
def test_config():       # Configuraci√≥n de prueba
def cleanup_files():     # Limpieza autom√°tica de archivos
```

## üìä Reportes y Cobertura

### Reportes Generados

1. **Reporte HTML de Pruebas**: `test_report.html`
   - Estado de cada prueba
   - Tiempos de ejecuci√≥n
   - Logs y capturas de errores

2. **Reporte de Cobertura HTML**: `htmlcov/index.html`
   - Cobertura por archivo
   - L√≠neas cubiertas/no cubiertas
   - An√°lisis detallado

3. **Reporte de Terminal**: Salida directa en consola

### Visualizaci√≥n de Reportes

```bash
# Abrir reporte de pruebas
start test_report.html        # Windows
open test_report.html         # Mac
xdg-open test_report.html     # Linux

# Abrir reporte de cobertura
start htmlcov/index.html      # Windows
open htmlcov/index.html       # Mac
xdg-open htmlcov/index.html   # Linux
```

## ‚öôÔ∏è Configuraci√≥n Avanzada

### pytest.ini

El archivo `pytest.ini` contiene configuraciones como:

```ini
[tool:pytest]
testpaths = tests
addopts = 
    --verbose
    --tb=short
    --cov=.
    --cov-report=html:htmlcov
    --cov-report=term-missing
    --maxfail=3
    --durations=10
filterwarnings =
    ignore::DeprecationWarning
markers =
    unit: pruebas unitarias
    integration: pruebas de integraci√≥n
    slow: pruebas lentas
    api: pruebas que requieren API
```

### Marcadores de Pruebas

```bash
# Solo pruebas unitarias
pytest -m unit

# Solo pruebas de integraci√≥n
pytest -m integration

# Excluir pruebas lentas
pytest -m "not slow"

# Solo pruebas de API
pytest -m api
```

### Variables de Entorno

```bash
# Configurar para testing
export TESTING=true
export REPLICATE_API_TOKEN=test_token

# Ejecutar con configuraci√≥n espec√≠fica
PYTEST_CURRENT_TEST=true pytest tests/
```

## üîß Desarrollo de Nuevas Pruebas

### Estructura de una Prueba

```python
import pytest
from unittest.mock import Mock, patch

class TestNuevoComponente:
    """Pruebas para nuevo componente"""
    
    def test_funcionalidad_basica(self, temp_dir):
        """Probar funcionalidad b√°sica"""
        # Arrange
        input_data = "test_input"
        expected = "expected_output"
        
        # Act
        result = nueva_funcion(input_data)
        
        # Assert
        assert result == expected
    
    @pytest.mark.api
    def test_integracion_api(self, mock_replicate):
        """Probar integraci√≥n con API externa"""
        # Configurar mock
        mock_replicate.run.return_value = {"status": "success"}
        
        # Ejecutar
        result = funcion_que_usa_api()
        
        # Verificar
        assert result["status"] == "success"
        mock_replicate.run.assert_called_once()
    
    @pytest.mark.slow
    def test_operacion_lenta(self):
        """Probar operaci√≥n que tarda tiempo"""
        # Esta prueba se puede saltar con -m "not slow"
        result = operacion_lenta()
        assert result is not None
```

### Mejores Pr√°cticas

1. **Nombres Descriptivos**: `test_calculate_cost_for_flux_pro_model`
2. **AAA Pattern**: Arrange, Act, Assert
3. **Un Assert por Prueba**: Enfoque en una sola verificaci√≥n
4. **Mocks para Dependencias**: Aislar la funcionalidad bajo prueba
5. **Datos de Prueba Realistas**: Usar fixtures con datos representativos

### Agregar Nueva Fixture

```python
# En conftest.py
@pytest.fixture
def mi_nueva_fixture():
    """Descripci√≥n de la fixture"""
    # Configuraci√≥n
    data = setup_test_data()
    yield data
    # Limpieza (opcional)
    cleanup_test_data(data)
```

## üêõ Depuraci√≥n de Pruebas

### Debugging con Pytest

```bash
# Ejecutar con pdb
pytest tests/test_archivo.py --pdb

# Mostrar prints
pytest tests/ -s

# Capturar solo las pruebas que fallan
pytest tests/ --tb=long --capture=no

# Ejecutar prueba espec√≠fica con m√°ximo detalle
pytest tests/test_archivo.py::test_funcion -v -s --tb=long
```

### Logging en Pruebas

```python
import logging

def test_con_logging(caplog):
    """Prueba que captura logs"""
    with caplog.at_level(logging.INFO):
        funcion_que_genera_logs()
    
    assert "mensaje esperado" in caplog.text
```

## üö® Soluci√≥n de Problemas

### Problemas Comunes

1. **ImportError**: Verificar que pytest est√© instalado en el entorno correcto
2. **Fallos de Mock**: Asegurarse de que los mocks coincidan con la API real
3. **Archivos Temporales**: Usar fixtures de limpieza para evitar conflictos
4. **Dependencias Faltantes**: Ejecutar `pip install -r requirements.txt`

### Verificaci√≥n de Salud

```bash
# Verificar instalaci√≥n
python -c "import pytest; print(pytest.__version__)"

# Verificar configuraci√≥n
pytest --collect-only

# Ejecutar una prueba simple
pytest tests/test_utils.py::TestUtilityFunctions::test_format_file_size -v
```

## üìà M√©tricas de Calidad

### Objetivos de Cobertura

- **Objetivo m√≠nimo**: 80% de cobertura de c√≥digo
- **Objetivo ideal**: 90%+ en componentes cr√≠ticos
- **Archivos cr√≠ticos**: `app.py`, m√≥dulos de c√°lculo de costos

### Monitoreo Continuo

```bash
# Ejecutar y generar badge de cobertura
pytest tests/ --cov=. --cov-fail-under=80

# Reporte de cobertura en formato XML (para CI)
pytest tests/ --cov=. --cov-report=xml
```

## üîÑ Integraci√≥n Continua

### GitHub Actions Example

```yaml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.10
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov
    - name: Run tests
      run: pytest tests/ --cov=. --cov-report=xml
    - name: Upload coverage
      uses: codecov/codecov-action@v1
```

---

## üéâ Conclusi√≥n

El sistema de testing proporciona una base s√≥lida para mantener la calidad del c√≥digo y detectar regresiones tempranamente. Con **40 pruebas automatizadas** y m√∫ltiples opciones de ejecuci√≥n, garantiza que el proyecto mantenga su robustez a medida que evoluciona.

**¬øPreguntas o problemas?** Consulta la documentaci√≥n de [pytest](https://docs.pytest.org/) o revisa los logs de las pruebas para m√°s detalles.

---
*√öltima actualizaci√≥n: Diciembre 2024*
